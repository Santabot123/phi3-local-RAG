{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35699f19-4c98-418a-b4d4-4e9cb755d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"unstructured[all-docs]\"\n",
    "!pip install chromadb\n",
    "!pip install langchain\n",
    "!pip install ollama\n",
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53473de4-fcd8-45e3-88a3-bb2aa07adf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_path='./data'\n",
    "\n",
    "def load_documents(data_path):\n",
    "    loader= DirectoryLoader(data_path, glob='*')\n",
    "    return loader.load()\n",
    "\n",
    "documents=load_documents(dir_path)\n",
    "# print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5986fdf-cf34-4524-86e5-b20561e96786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_text(documents:list):\n",
    "    text_spliter= RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=500,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    chunks=text_spliter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "chunks=split_text(documents)\n",
    "# print(chunks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90f78dba-bb47-4959-af5f-79b82634c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "ollama_emb = OllamaEmbeddings(model=\"phi3\")\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=ollama_emb,persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ef5b96e-1996-4283-aede-e1ee887d15e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided data in Table 6, which shows word class frequency by age for different categories such as \"money job,\" \"sports,\" \"tv,\" \"sleep,\" \"eating,\" \"sex,\" \"family,\" \"friends,\" \"pos-emotions,\" and \"neg-emotions,\" we can draw several conclusions regarding language use across ages. However, it's essential to note that the data presented includes standard errors (±0.5 or ±1.9) alongside mean values for each category per age group. This allows us to understand not just central tendencies but also variability and reliability of these measures.\n",
      "\n",
      "Here are some general observations:\n",
      "\n",
      "1. \"Eating\" related terms seem more prominent in the younger (20s and 30s) groups, with noticeable declines as age increases. This might suggest that food-related language becomes less prevalent or shifts towards different topics as individuals mature.\n",
      "   \n",
      "2. \"Sex\" related words are notably higher among middle-aged adults (30s), which could reflect cultural attitudes and societal openness to discussing this topic more freely during that life stage. However, the high standard error implies variability within these age groups as well.\n",
      "   \n",
      "3. Emotional terms (\"pos-emotions\" and \"neg-emotions\") show a general increase with age, possibly indicating greater emotional expression or reflection in older ages. This pattern is consistent across all three decades of adulthood (20s, 30s, and 40s), suggesting that these word classes become more prevalent as people grow older.\n",
      "   \n",
      "4. \"Family\" related words are most prominent among adults in their 30s, which aligns with the typical life stage where individuals may have a family but not necessarily be parents (e.g., grown children). The standard error suggests variability across this age group as well.\n",
      "   \n",
      "5. Categories like \"money job\" and \"sleep\" show relatively consistent frequency patterns across different ages, suggesting these topics maintain relevance irrespective of life stage.\n",
      "\n",
      "The studies referenced (e.g., Mulac et al., 2001; Pennebaker & Stone, 2003) provide a broader context to this data by examining how gender differences may influence language use over the lifespan and across cultures. The research indicates that while certain patterns emerge based on age or other factors like culture and gender, individual variation exists within these categories.\n",
      "\n",
      "In conclusion, our analysis suggests that word class frequency varies with age, reflecting potential changes in interests, concerns, and societal roles as people progress through different stages of life. However, it's crucial to interpret these findings cautiously, considering the standard errors indicating variability within each age group."
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler \n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use three sentences maximum and keep the answer as concise as possible.\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "llm = Ollama(model=\"phi3\", \n",
    "             callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")\n",
    "\n",
    "\n",
    "YOUR_QUESTION = 'What is the conclusion?'\n",
    "result = qa_chain({\"query\": YOUR_QUESTION })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00307e9-8ffe-4ba7-ac82-d0d50e728c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
